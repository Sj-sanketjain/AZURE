Optimized file formats

While human-readable formats for structured and semi-structured data can be useful, they're typically not optimized for storage space or processing. 
Over time, some specialized file formats that enable compression, indexing, and efficient storage and processing have been developed.

Some common optimized file formats you might see include Avro, ORC, and Parquet:

ğŸ§± Avro

1.) Row-based format
2.) Great for data streaming and schema evolution
3.) Often used with Kafka
4.) Stores schema with the data
5.) Not optimized for analytics scans

Avro is a row-based format. It was created by Apache. Each record contains a header that describes the structure of the data in the record. This header is stored as JSON.
The data is stored as binary information. An application uses the information in the header to parse the binary data and extract the fields it contains. 
Avro is a good format for compressing data and minimizing storage and network bandwidth requirements.

ğŸ“Š ORC (Optimized Row Columnar)

1.) Columnar format
2.) Built for high-performance analytics
3.) Strong compression and indexing
4.) Originally from the Hadoop ecosystem (Hive-heavy)

ORC (Optimized Row Columnar format) organizes data into columns rather than rows. 
It was developed by HortonWorks for optimizing read and write operations in Apache Hive (Hive is a data warehouse system that supports fast data summarization and querying over large datasets). 
An ORC file contains stripes of data. Each stripe holds the data for a column or set of columns. 
A stripe contains an index into the rows in the stripe, the data for each row, and a footer that holds statistical information (count, sum, max, min, and so on) for each column.

ğŸ“¦ Parquet

1.) Columnar format (like ORC)
2.) Designed for analytics + broad ecosystem support
3.) Excellent compression & predicate pushdown
4.) Very popular with Spark, Presto, Athena, BigQuery, etc.

Parquet is another columnar data format. It was created by Cloudera and X. A Parquet file contains row groups. 
Data for each column is stored together in the same row group. Each row group contains one or more chunks of data. 
A Parquet file includes metadata that describes the set of rows found in each chunk. 
An application can use this metadata to quickly locate the correct chunk for a given set of rows, and retrieve the data in the specified columns for these rows.
Parquet specializes in storing and processing nested data types efficiently. 
It supports very efficient compression and encoding schemes.

How theyâ€™re actually related

Avro vs Parquet/ORC â†’ row-based vs columnar
Parquet vs ORC â†’ both columnar, different implementations
Parquet does not include Avro or ORC
You can convert Avro â†’ Parquet or ORC (very common)

Simple mental model

Avro = transport / streaming
Parquet & ORC = analytics / storage

1ï¸âƒ£ Choose Avro when you care about data movement & evolution

Use Avro if:
Data is streaming or event-based
Schema changes frequently
You need backward/forward compatibility
Youâ€™re moving data between systems

Example:

Kafka â†’ Microservices â†’ Data Lake (raw layer)

UserSignupEvent
{
  user_id: string,
  email: string,
  signup_time: timestamp
}

Old consumers still work â†’ âœ… Avro handles this cleanly.

ğŸ“Œ Typical use cases

Kafka topics
CDC (Debezium)
API event logs
â€œBronze / Rawâ€ data layer

âŒ Not ideal for analytics queries like:

â€œcount signups per country last 30 daysâ€

2ï¸âƒ£ Choose Parquet for general-purpose analytics (most common choice)

Use Parquet if:

You run Spark / Trino / Athena / BigQuery
Queries read only some columns
You want good compression + portability
You donâ€™t want engine lock-in

Example:

S3 data lake â†’ Spark SQL

SELECT country, COUNT(*)
FROM users
WHERE signup_date >= '2025-01-01'
GROUP BY country;


Parquet reads only:

country
signup_date

ğŸš€ Fast and cheap.
ğŸ“Œ Typical use cases

Data lakes
ETL outputs
Feature stores
BI dashboards

âœ… This is the default recommendation in most architectures.

3ï¸âƒ£ Choose ORC for maximum query performance (Hive-heavy stacks)

Use ORC if:

Youâ€™re deep in Hive
Queries are very large scans
You want aggressive compression & indexes
You control the full stack

Example:

Hive warehouse with petabytes of data

SELECT SUM(revenue)
FROM transactions
WHERE year = 2024 AND country = 'US';


ORC:

Uses built-in indexes
Skips huge chunks of data
Often slightly faster than Parquet in Hive

ğŸ“Œ Typical use cases

Enterprise Hadoop warehouses
Long-running analytical queries
Very large tables with stable schemas

âŒ Less flexible across tools than Parquet.

ğŸ§  Quick decision table
Situation	Choose
Streaming events	Avro
Kafka / CDC	Avro
Data lake analytics	Parquet âœ…
Spark / Athena / Trino	Parquet
Hive-only warehouse	ORC
Schema changes often	Avro
BI / dashboards	Parquet


ğŸ—ï¸ Real-world pipeline example

Kafka (Avro)
   â†“
Raw zone (Avro)
   â†“
Cleaned / Curated zone (Parquet)
   â†“
Analytics / BI (Parquet)
